# User Flow: Ollama Integration - Detailed Interactions

**Date**: January 5, 2026  
**Status**: Implementation Guide  

## Complete User Journeys

---

## Journey 1: First-Time Ollama User (5-10 minutes)

### Context
User has Ollama running locally (`http://localhost:11434`) with llama3.2 model, but hasn't configured it in the app yet.

### Step-by-Step Flow

#### Step 1: User Opens App
- **Current State**: App loads, user views library
- **What They See**: Normal library screen with settings icon
- **User Action**: Taps Settings icon

#### Step 2: Settings Screen Opens
- **Current State**: User scrolls down to "Ollama Configuration" section (new)
- **What They See**:
  ```
  Ollama Configuration
  Status: ðŸ”´ Not Configured
  [Set Up Ollama]
  ```
- **User Action**: Taps "Set Up Ollama" button

#### Step 3: Configuration Modal Opens
- **Current State**: Modal shows Ollama setup form
- **What They See**:
  ```
  Configure Ollama Server
  
  Server URL:
  [http://localhost:11434] â† pre-filled
  
  [Test Connection]
  ```
- **User Action**: Sees pre-filled URL (good default), taps "Test Connection"

#### Step 4: Connection Test
- **Current State**: App attempts to reach Ollama
- **What They See**:
  ```
  [Testing...] (spinner)
  ```
- **Expected Duration**: 1-2 seconds
- **What Happens Next**: 
  - âœ… **If Connected**: 
    ```
    âœ“ Connected! Available models:
    â€¢ llama3.2 (11B) â† auto-selected
    â€¢ qwen2.5 (7B)
    [Save Configuration]
    ```
  - âŒ **If Failed**: 
    ```
    âœ— Connection failed: Connection refused
    
    Is Ollama running?
    Check: http://localhost:11434 is accessible
    
    [Retry] [Use Different URL]
    ```

#### Step 5: Model Selection
- **Current State**: Success message shown, models listed
- **What They See**: llama3.2 is pre-selected (first available model)
- **User Action**: Accepts default or taps another model (e.g., qwen2.5)

#### Step 6: Save Configuration
- **Current State**: User reviews settings
- **User Action**: Taps "Save Configuration"
- **What Happens**: 
  - Settings saved to SharedPreferences
  - Modal closes
  - Returns to Settings screen

#### Step 7: Settings Screen Updated
- **Current State**: Ollama configuration now shows:
  ```
  Ollama Configuration
  Status: ðŸŸ¢ Connected to localhost:11434
  Model: llama3.2
  Last checked: Just now
  ```
- **User Action**: Navigates back to Library

#### Step 8: Generate Content
- **Current State**: User opens a book, views generate button
- **User Action**: Taps "Generate Content" button
- **What They See**: Mode selector modal:
  ```
  Choose Generation Method:
  
  [ðŸ“‹ Paste from ChatGPT]
  Use your preferred AI assistant
  
  [ðŸ¤– Ollama (Connected)]
  Local generation - llama3.2
  ```
- **User Action**: Taps "ðŸ¤– Ollama (Connected)" button

#### Step 9: Ollama Generation
- **Current State**: Generation starts
- **What They See**: 
  ```
  âŸ³ Generating content...
  
  Using: llama3.2 (local)
  Tokens: 145 / â€”
  Time: 3.2s
  
  [Content streaming in...]
  
  [Cancel]
  ```
- **Expected Duration**: 5-20 seconds depending on model and system

#### Step 10: Result Display
- **Current State**: Generation complete
- **What They See**:
  ```
  [Generated content...]
  
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ðŸ¤– Generated by: llama3.2 (11B)
  Generation time: 8.4s
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  [Edit] [âœ“ Accept] [â†» Regenerate]
  [Try Different Method]
  ```
- **User Action**: Taps "âœ“ Accept" to use the content

#### Step 11: Content Accepted
- **Current State**: Content added to book
- **What They See**: Success feedback, returns to book view
- **Final Result**: User now has Ollama generation working and can use it repeatedly

---

## Journey 2: Switching Between Modes (When One Fails)

### Context
User is familiar with app, has both copy-paste and Ollama available.

### Step-by-Step Flow

#### Step 1: Try Ollama First
- **Current State**: User taps "Generate" button
- **User Action**: Selects "ðŸ¤– Ollama" mode

#### Step 2: Generation Starts
- **Current State**: Streaming begins
- **What They See**: Loading spinner, token count

#### Step 3: Connection Fails Mid-Generation
- **Current State**: Ollama server was restarted, connection drops
- **What They See**:
  ```
  âš ï¸ Connection Lost
  
  Ollama server at localhost:11434 
  became unavailable during generation.
  
  [Retry] [Use Copy-Paste Instead]
  ```
- **User Action**: Taps "Use Copy-Paste Instead" (doesn't want to wait for server)

#### Step 4: Switch to Copy-Paste
- **Current State**: Modal dismisses, paste input view appears
- **What They See**:
  ```
  Generate Content (Copy-Paste Mode)
  
  Paste response from ChatGPT:
  [____________ 
   __________
   ________]
  
  [Paste & Generate] [Cancel]
  
  Or open ChatGPT and share:
  [Open ChatGPT]
  ```
- **User Action**: Opens ChatGPT app, generates content, copies it, returns to app and pastes

#### Step 5: Content Accepted
- **Current State**: Paste mode processes content
- **What They See**: Content accepted, added to book

#### Step 6: Later: Ollama Back Online
- **Current State**: User restarts Ollama, returns to app
- **What They See**: 
  - Settings now show: "Status: ðŸŸ¢ Connected (was offline)"
  - Next time they tap "Generate", Ollama is available again
- **User Action**: Uses Ollama mode again

---

## Journey 3: Configuring Remote Ollama Server

### Context
User has Ollama running on their home server (192.168.1.100:11434), not localhost.

### Step-by-Step Flow

#### Step 1: Settings
- **User Action**: Opens Settings â†’ Ollama Configuration

#### Step 2: URL Configuration
- **Current State**: Connection status shows offline
- **What They See**:
  ```
  Ollama Configuration
  Status: ðŸ”´ Not Configured
  
  Server URL:
  [http://localhost:11434] â† needs to change
  
  Tip: Include protocol (http://) and port
  Example: http://192.168.1.100:11434
  
  [Test Connection]
  ```
- **User Action**: Clears field and enters: `http://192.168.1.100:11434`

#### Step 3: Test Connection
- **User Action**: Taps "Test Connection"
- **What Happens**: App connects to remote server

#### Step 4: Success
- **What They See**:
  ```
  âœ“ Connected! Available models:
  â€¢ llama3.2 (11B)
  â€¢ qwen2.5 (7B)
  â€¢ mistral (7B)
  
  [Save Configuration]
  ```
- **User Action**: Taps "Save Configuration"

#### Step 5: Ready to Generate
- **Current State**: Settings updated, can now generate
- **What They See**: Next time they generate, Ollama is available via remote server
- **Benefit**: Same smooth experience whether local or remote

---

## Journey 4: Error Recovery - Invalid Configuration

### Context
User enters incorrect URL or model doesn't exist.

### Scenario A: Invalid URL Format

#### Step 1: User Enters Invalid URL
- **Current State**: Settings screen
- **User Action**: Enters: `https://my-ollama` (missing port)
- **What They See**: Error indicator appears inline:
  ```
  Server URL:
  [https://my-ollama] âš ï¸
  URL format error: Port required
  Example: http://192.168.1.100:11434
  ```

#### Step 2: User Corrects
- **User Action**: Fixes to: `http://192.168.1.100:11434`
- **What They See**: Error clears, "Test Connection" is enabled again

#### Step 3: Test and Save
- **User Action**: Taps "Test Connection", then "Save"
- **Result**: Configuration saved

### Scenario B: Server Not Reachable

#### Step 1: Valid URL, Server Offline
- **Current State**: User enters correct URL
- **User Action**: Taps "Test Connection"
- **What They See**:
  ```
  [Testing...] (spinner)
  ```

#### Step 2: Connection Failure
- **What They See**:
  ```
  âœ— Connection failed
  
  Server at http://192.168.1.100:11434 
  did not respond (timeout after 5s).
  
  Possible causes:
  â€¢ Ollama is not running
  â€¢ Firewall is blocking connection
  â€¢ IP address or port is incorrect
  
  [Retry] [Check Ollama Status] [Use Different URL]
  ```

#### Step 3: Resolution Options
- **User Action**: 
  - Taps "Check Ollama Status" â†’ Opens Ollama status info
  - Or taps "Retry" â†’ Retries connection
  - Or taps "Use Different URL" â†’ Edits URL again

#### Step 4: Once Fixed
- **What They See**: Connection succeeds, models load
- **User Action**: "Save Configuration"

---

## Journey 5: Model Selection & Switching

### Context
User wants to compare different models or has multiple installed.

### Step-by-Step Flow

#### Step 1: Open Model Selector
- **Current State**: Settings â†’ Ollama Configuration
- **What They See**:
  ```
  Model Selection
  Current: llama3.2 (11B, 7.9B downloaded)
  
  Available Models:
  â”œâ”€ llama3.2 (11B) [Currently selected]
  â”‚  Capabilities: Tool calling, Vision, Fast
  â”‚  Downloaded: 7.9GB
  â”‚
  â”œâ”€ qwen2.5 (7B)
  â”‚  Capabilities: Tool calling, Coding-optimized
  â”‚  Downloaded: 4.2GB
  â”‚
  â”œâ”€ mistral (7B)
  â”‚  Not downloaded - [Pull model]
  â”‚
  â””â”€ phi4 (14B)
      Not downloaded - [Pull model]
  
  [Manage Models (Ollama Web UI)]
  ```

#### Step 2: Compare Models
- **What They See**: Size, capabilities, download status for each model
- **User Action**: Reads descriptions to understand differences

#### Step 3: Switch Model
- **Current State**: User wants to try qwen2.5
- **User Action**: Taps "qwen2.5 (7B)"
- **What Happens**: 
  - Model selection updated
  - Settings saved immediately
  - Next generation uses qwen2.5

#### Step 4: Next Generation
- **Current State**: User generates content
- **What They See**:
  ```
  Choose Generation Method:
  
  [ðŸ“‹ Paste from ChatGPT]
  
  [ðŸ¤– Ollama (Connected)]
  Local generation - qwen2.5 â† changed!
  ```

#### Step 5: Pull New Model (Optional)
- **Context**: User wants to use "mistral" but it's not downloaded
- **Current State**: User taps "Pull model" for mistral
- **What They See**:
  ```
  Pulling mistral (7B)...
  
  Progress: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 82%
  Downloaded: 5.2GB / 6.3GB
  Time remaining: ~2 min
  
  [Cancel Pull]
  ```
- **Expected Duration**: 2-5 minutes depending on internet speed
- **Once Complete**: Model available for generation

---

## Journey 6: Generation with Streaming Feedback

### Context
User generates with Ollama and sees real-time progress.

### Step-by-Step Flow

#### Step 1: Start Generation
- **User Action**: Selects Ollama mode, taps "Generate"

#### Step 2: Immediate Feedback
- **Current State**: Generation begins (no initial delay for model loading)
- **What They See**:
  ```
  âŸ³ Generating content...
  
  Using: llama3.2 (local)
  Status: Processing... [â—] 23% complete
  Time: 2.4s
  
  [Content starting to appear here...]
  ```

#### Step 3: Streaming Content
- **Current State**: Content appears word-by-word/chunk-by-chunk
- **What They See**:
  ```
  âŸ³ Generating content...
  
  Using: llama3.2 (local)
  [âœ“ Processing] Status: Streaming response
  
  The evolution of climate change understanding began in the 
  early 20th century when scientists first proposed the 
  greenhouse gas effect. Initial observations were limited to 
  [more content continues to appear...]
  
  Tokens generated: 145/500 (est.)
  Time: 8.2s
  Speed: ~17 tokens/sec
  
  [Stop Generation]
  ```

#### Step 4: Generation Complete
- **What They See**: Content finishes streaming
  ```
  âŸ³ Generation complete
  
  Generated 367 tokens in 21.4 seconds
  Average speed: 17 tokens/second
  
  [Full content visible...]
  ```

#### Step 5: Result Actions
- **What They See**:
  ```
  [Generated content...]
  
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ðŸ¤– Generated by: llama3.2 (11B)
  Generation time: 21.4s
  Performance: 17 tokens/sec
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  
  [Edit] [âœ“ Accept] [â†» Regenerate]
  [Try Different Method]
  [Copy to Clipboard]
  [Share Generation Method]
  ```

#### Step 6: User Action
- **Options**:
  - **Edit**: Opens edit dialog to modify content before saving
  - **Accept**: Adds content to book, records it was Ollama-generated
  - **Regenerate**: Runs generation again with same settings
  - **Try Different Method**: Switches to copy-paste without losing progress
  - **Copy**: Copies generated text for use elsewhere
  - **Share**: Shares the generation flow (educational)

---

## Journey 7: Offline Graceful Degradation

### Context
User loses Ollama connection mid-workflow.

### Timeline

#### T=0: App Open (Ollama Online)
- Settings show: "ðŸŸ¢ Connected"
- Mode selector shows: "ðŸ¤– Ollama (Connected)"

#### T=60 seconds: User Starts Generation
- User selects Ollama mode
- Generation begins successfully

#### T=90 seconds: Server Goes Offline
- User restarts Ollama for updates
- Generation times out

#### T=95 seconds: Error Handled
- **What User Sees**:
  ```
  âš ï¸ Generation Stopped
  
  The Ollama server at localhost:11434 
  became unavailable.
  
  Last successful connection: 1 min ago
  
  Options:
  [Retry] [Use Copy-Paste Mode] [Check Settings]
  ```

#### Step: User Chooses Fallback
- **User Action**: Taps "Use Copy-Paste Mode"
- **What Happens**:
  - Modal closes
  - Paste input appears
  - User can immediately switch to ChatGPT without losing context
  - No app crash or complex error handling needed

#### Step: User Continues
- **User Action**: Uses ChatGPT, copies response, pastes
- **Result**: Content accepted, work continues

#### Step: Later - Connection Restored
- **Context**: User restarts Ollama
- **What They See**: 
  - User opens app again
  - Settings check connection
  - Status updates: "ðŸŸ¢ Connected" (was briefly offline)
  - Next generation option shows Ollama available again

---

## Key Interaction Principles

### 1. **One-Tap Mode Switch**
Switching from Ollama to copy-paste (or vice versa) should never require more than 1 tap plus confirmation.

### 2. **Immediate Feedback**
All actions should provide visual feedback within 100ms:
- Button press â†’ pressed state
- Connection test â†’ "Testing..." spinner
- Generation start â†’ loading UI

### 3. **Clear Status**
Connection status must always be visible where needed:
- Settings: Always visible in Ollama Configuration section
- Generate dialog: Shown in mode selector buttons
- Generate screen: Shown in loading state

### 4. **Helpful Errors**
Every error should include:
- What went wrong (specific, not generic)
- Why it happened (possible causes)
- How to fix it (actionable steps)

### 5. **Smart Defaults**
- URL: Pre-fill with localhost:11434 (most common)
- Model: Auto-select first available model
- Mode: Remember last used mode, suggest in dialog

### 6. **No Modal Hell**
- Never nest more than 2 modals deep
- Always provide back/cancel option
- Large modals: Close by tapping outside (if not destructive)

---

## Accessibility Notes

### For Each Journey Step:

**Journey 1, Step 2**: 
- Screen reader: "Settings, button"
- Focus order: Correct navigation hierarchy
- Touch target: 48dp minimum

**Journey 1, Step 3**:
- Label: "Configure Ollama Server"
- Input: "Server URL, text field"
- Button: "Test Connection, button"
- Screen reader reads placeholder: "localhost:11434"

**Journey 2, Step 4**:
- Error accessible: "Status, offline, Ollama server at localhost:11434 became unavailable"
- Buttons remain interactive
- Text contrast > 4.5:1

### Testing Checklist:
- [ ] All steps navigable with keyboard only
- [ ] Screen reader can announce status changes
- [ ] Error messages read clearly
- [ ] Button labels are descriptive
- [ ] No color-only indicators

---

**Document Version**: 1.0  
**Last Updated**: January 5, 2026
